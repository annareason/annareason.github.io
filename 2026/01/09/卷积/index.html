<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>卷积 | 维娜丝小课堂</title><meta name="author" content="Cx330"><meta name="copyright" content="Cx330"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="1.数据预处理：&#160; &#160; &#160; &#160;数据归一化将图像的像素值归一化到一个特定的范围,如代码中的transform不仅将数据储存为tensor，同时也把灰度数据范围从$[0,255]$转换到$[0,1]$之间，而之后也进行了通道的正则化处理。&#160; &#160; &#160; &#160;归一化有助于加快网络的训练速度。因为不同的图像像素值范围可能差异很大，而">
<meta property="og:type" content="article">
<meta property="og:title" content="卷积">
<meta property="og:url" content="http://example.com/2026/01/09/%E5%8D%B7%E7%A7%AF/index.html">
<meta property="og:site_name" content="维娜丝小课堂">
<meta property="og:description" content="1.数据预处理：&#160; &#160; &#160; &#160;数据归一化将图像的像素值归一化到一个特定的范围,如代码中的transform不仅将数据储存为tensor，同时也把灰度数据范围从$[0,255]$转换到$[0,1]$之间，而之后也进行了通道的正则化处理。&#160; &#160; &#160; &#160;归一化有助于加快网络的训练速度。因为不同的图像像素值范围可能差异很大，而">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://example.com/img/butterfly-icon.png">
<meta property="article:published_time" content="2026-01-09T02:53:15.000Z">
<meta property="article:modified_time" content="2026-01-09T09:39:20.601Z">
<meta property="article:author" content="Cx330">
<meta property="article:tag" content="机器学习,论文,Python">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/img/butterfly-icon.png"><script type="application/ld+json">{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "卷积",
  "url": "http://example.com/2026/01/09/%E5%8D%B7%E7%A7%AF/",
  "image": "http://example.com/img/butterfly-icon.png",
  "datePublished": "2026-01-09T02:53:15.000Z",
  "dateModified": "2026-01-09T09:39:20.601Z",
  "author": [
    {
      "@type": "Person",
      "name": "Cx330",
      "url": "http://example.com"
    }
  ]
}</script><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="http://example.com/2026/01/09/%E5%8D%B7%E7%A7%AF/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css?v=5.5.2"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@7.1.0/css/all.min.css"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":false,"top_n_per_article":1,"unescape":false,"pagination":{"enable":false,"hitsPerPage":8},"languages":{"hits_empty":"未找到符合您查询的内容：${query}","hits_stats":"共找到 ${hits} 篇文章"}},
  translate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false,"highlightFullpage":false,"highlightMacStyle":false},
  copy: {
    success: '复制成功',
    error: '复制失败',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'null',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.12.0/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyloadPlugin: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '卷积',
  isHighlightShrink: false,
  isToc: true,
  pageType: 'post'
}</script><meta name="generator" content="Hexo 7.3.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img text-center"><img src="/img/butterfly-icon.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data text-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">2</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">0</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">0</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fa fa-graduation-cap"></i><span> 博文</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/categories/"><i class="fa-fw fa fa-archive"></i><span> 分类</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></li><li><a class="site-page child" href="/archives/"><i class="fa-fw fa fa-folder-open"></i><span> 归档</span></a></li></ul></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fas fa-list"></i><span> 生活</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/shuoshuo/"><i class="fa-fw fa fa-comments-o"></i><span> 分享</span></a></li><li><a class="site-page child" href="/photos/"><i class="fa-fw fa fa-camera-retro"></i><span> 相册</span></a></li><li><a class="site-page child" href="/music/"><i class="fa-fw fa fa-music"></i><span> 音乐</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 影视</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/links/"><i class="fa-fw fa fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/comment/"><i class="fa-fw fa fa-paper-plane"></i><span> 留言板</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于笔者</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><span class="site-name">维娜丝小课堂</span></a><a class="nav-page-title" href="/"><span class="site-name">卷积</span><span class="site-name"><i class="fa-solid fa-circle-arrow-left"></i><span>  返回首页</span></span></a></span><div id="menus"><div id="search-button"><span class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></span></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fa fa-graduation-cap"></i><span> 博文</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/categories/"><i class="fa-fw fa fa-archive"></i><span> 分类</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></li><li><a class="site-page child" href="/archives/"><i class="fa-fw fa fa-folder-open"></i><span> 归档</span></a></li></ul></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fas fa-list"></i><span> 生活</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/shuoshuo/"><i class="fa-fw fa fa-comments-o"></i><span> 分享</span></a></li><li><a class="site-page child" href="/photos/"><i class="fa-fw fa fa-camera-retro"></i><span> 相册</span></a></li><li><a class="site-page child" href="/music/"><i class="fa-fw fa fa-music"></i><span> 音乐</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 影视</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/links/"><i class="fa-fw fa fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/comment/"><i class="fa-fw fa fa-paper-plane"></i><span> 留言板</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于笔者</span></a></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div></div></nav><div id="post-info"><h1 class="post-title">卷积</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2026-01-09T02:53:15.000Z" title="发表于 2026-01-09 10:53:15">2026-01-09</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2026-01-09T09:39:20.601Z" title="更新于 2026-01-09 17:39:20">2026-01-09</time></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">浏览量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="container post-content" id="article-container"><p><strong>1.数据预处理：</strong><br>&#160; &#160; &#160; &#160;数据归一化将图像的像素值归一化到一个特定的范围,如代码中的transform不仅将数据储存为tensor，同时也把灰度数据范围从$[0,255]$转换到$[0,1]$之间，而之后也进行了通道的正则化处理。<br>&#160; &#160; &#160; &#160;归一化有助于加快网络的训练速度。因为不同的图像像素值范围可能差异很大，而归一化将输入数据的分布调整到一个更标准的范围，使得不同特征之间的尺度变得一致，这有助于模型更快地收敛，避免梯度消失或梯度爆炸问题。<br>&#160; &#160; &#160; &#160;而对于归一化层也有多方法：<br><img src="/image-11.png" alt="alt text"></p>
<p><strong>2.卷积层：</strong><br>&#160; &#160; &#160; &#160;用来提取输入数据的局部特征，通过卷积核对输入数据进行局部运算（卷积核在输入数据上滑动），计算每个局部区域的加权和，具体公式如下：$Y(i, j) &#x3D; \sum_{m} \sum_{n} X(i+m, j+n) \cdot W(m, n) + b $。<br>&#160; &#160; &#160; &#160;而在代码中，其所接受的参数主要有：in_channel(输入特征的通道量)，out_channel(输出特征的通道量)，kerner_size(卷积核大小)，stride(步幅)，padding(填充),这里的kerner_size通常为奇数。<br>&#160; &#160; &#160; &#160;卷积操作能够自动提取图像中的局部特征，大大减少了模型的参数数量，降低了计算量和过拟合的风险。<br><img src="/image-5.png" alt="alt text"></p>
<p><strong>3.激活函数：</strong><br>&#160; &#160; &#160; &#160;常见的激活函数有Sigmoid函数，Rule函数，Tanh函数等等。<br>&#160; &#160; &#160; &#160;用于增加模型的非线性能力，帮助卷积层和全连接层更好地学习复杂的模式，简单而言就是使模型变“深”。<br><img src="/image-8.png" alt="alt text"></p>
<p><strong>4.池化层：</strong><br>&#160; &#160; &#160; &#160;主要是用来去除一些不重要的特征，保留重要特征，这样可以减少参数矩阵的尺寸，提高计算率，也可以防止过细节化导致过拟合。<br>&#160; &#160; &#160; &#160;由于池化操作是更加关注某些特征的存在。那么便有了不变形性：平移不变性、旋转不变性和尺度不变性。<br>&#160; &#160; &#160; &#160;在这里再引入汇聚层：其主要作用是对卷积层输出的特征图进行下采样。通常有两种方式：最大池化（Max Pooling）和平均池化当然，总体上其代码参数与卷积层类似，但多出一个ceil_mode（向上取整，可避免数据丢失）</p>
<ul>
<li>平均池化（average pooling）：计算图像区域的平均值作为该区域池化后的值</li>
<li>最大池化（max pooling）:选图像区域的最大值作为该区域池化后的值，但比average pooling多出参数divisor_override（决定以什么为除数）</li>
</ul>
<p>&#160; &#160; &#160; &#160;池化操作可以进一步减少数据量，降低计算成本，而由于其具有的平移不变性，这也提高了模型的鲁棒性（鲁棒性指模型的稳定性）。<br><img src="/image-7.png" alt="alt text"></p>
<p><strong>5.全连接层和输出层：</strong><br>&#160; &#160; &#160; &#160;全连接层通俗理解就是连接各个层的一个层以及最后一层实现对输入数据的分类（这里其实也是输出层），它将输入和输出的神经元相连，通过对输入数据的线性变换与激活函数的非线性变换将高维特征量压缩或映射到目标维度，或通过Softmax函数输出每个类别的概率分布，从而实现对输入数据的分类,具体公式为$y &#x3D; \sigma(Wx + b)$。<br>&#160; &#160; &#160; &#160;全连接层可以综合前面提取的所有特征，进行分类或回归等任务，而输出层的 Softmax 概率分布可以直观地表示图像属于各个类别的可能性。</p>
<p><strong>6.输出形状的公式：</strong><br>&#160; &#160; &#160; &#160;而对与卷积层，池化层的各个参数综合来说，特征的输出尺寸为$Output Size &#x3D; [(Hin + 2*P - F)&#x2F;S] + 1$。其中$F$是卷积核的大小,$P$是填充的数量,$S$是步长,这里的[  ]表示向上取整。</p>
<p><strong>7.互相关运算：</strong><br>&#160; &#160; &#160; &#160;基于滤波器的图像处理方法，对图像进行平滑和增强，也就是将卷积应用于每个像素点进而产生图像。<br>&#160; &#160; &#160; &#160;具体数学公式$(f \star g)[n] &#x3D; \sum_{m&#x3D;-\infty}^{\infty} f[m] \cdot g[n+m]$</p>
<p><strong>8.步幅和填充：</strong><br>&#160; &#160; &#160; &#160;这里设置步幅的目的：希望减小输入参数的数目，减少计算量；设置填充的目的：希望每个输入方块都能作为卷积窗口的中心，减少丢失<br>&#160; &#160; &#160; &#160;这里提一下padding的分类</p>
<ul>
<li>padding&#x3D;’same’表示进行填充，填充的值由算法内部根据卷积核大小计算，目的是让输出尺寸和输入相等；</li>
<li>padding&#x3D;’valid’表示不进行填充，即是 padding&#x3D;0，只使用有效的窗口位置，这是默认的选项。其填充的值&#x3D;(b - 1)&#x2F;2<br>（这里也变相解释了为什么卷积核大小为奇数）。</li>
<li>当然，这里也可以自己填充，但只可输入整数（代表在各个方向上的填充是相等的）或元组</li>
</ul>
<p><strong>9.损失函数：</strong></p>
<ul>
<li>均方误差损失函数（MSE）：主要用于回归问题<br><img src="/image-4.png" alt="alt text"></li>
<li>交叉熵损失函数：主要用于回归问题<br><img src="/%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE%202024-11-09%20163634.png" alt="alt text"><br><img src="/image-10.png" alt="alt text"></li>
</ul>
<p><strong>10.in_channel:</strong><br>&#160; &#160; &#160; &#160;in_channe通常指输入数据的深度和特征的属性<br>&#160; &#160; &#160; &#160;对二维图像所言，输入通道数指的是每个像素点有几个数值：如彩色图像，每个像素点由三个通道组成（红绿蓝），故彩色图像的in_channel为3，而对灰度图像来说，便只有一个通道。<br>&#160; &#160; &#160; &#160;类比颜色，in_channel其实还代表了图像的特征，如人脸图像中的鼻子特征，眼睛特征等等（如果卷积层有多个通道，每个通道的滤波器将分别提取数据在不同方面的特征，进而产生多个输出特征图）<br>【明确说就是特征，只不过在输入的彩色图像中是RGB三通道，颜色也可算作是通道，经过卷积后输出的通道可以是边缘、纹理、形状、物体部分等，而不仅仅局限于输入图像的颜色信息。】</p>
<p><strong>11.out_channel:</strong><br>&#160; &#160; &#160; &#160;out_channe一般对应卷积核的个数，即提取的特征数（因为一个卷积核提取一种特征），每个卷积核生成的特征特征图汇总起来成为输出特征图。</p>
<p><strong>12.optimzer中的Momentum:</strong><br>&#160; &#160; &#160; &#160;momentum用于加速SGD（随机梯度下降）训练，并减少震荡（这一作用与引入高斯噪声相似）。<br>&#160; &#160; &#160; &#160;具体更新公式（简化形式）类似于$v &#x3D; momentum * v - lr * grad和param &#x3D; param + v，其中v是速度$。<br>&#160; &#160; &#160; &#160;动量通过在参数更新项中加上一次更新量（即动量项），使得在更新模型参数时，对于当前梯度方向与上一次梯度方向相同的参数进行加强，即这些方向上更快了(此时v和grad的方向相反)；对于当前梯度方向与上一次梯度方向不同的参数进行削减，即这些方向上减慢了(此时v和grad的方向相反)。这样，动量可以帮助梯度下降算法更快地收敛到最优解，并减少在局部极值点附近的震荡。<br>&#160; &#160; &#160; &#160;通俗理解可以为在一条路上要换不同的司机开车，有momentum的司机会借鉴之前司机的建议，当遇到局部最小值时，他觉得应该停下但其他司机不让，那么他也不会停下；反之，没momentum的司机就是一意孤行。</p>
<p><strong>13.1*1卷积核:</strong><br>作用：</p>
<ul>
<li>增加网络深度（增加非线性映射次数）<br>&#160; &#160; &#160; &#160;在保持feature map尺度不变的（即不损失分辨率）的前提下大幅增加非线性特性（利用后接的非线性激活函数），把网络做的很深。并且1x1卷积核的卷积过程相当于全连接的计算过程，通过加入非线性激活函数，可以增加网络的非线性，使得网络可以表达更复杂的特征。</li>
<li>升维&#x2F;降维<br>&#160; &#160; &#160; &#160;即通道数的变化，也就是改变卷积核的数量实现。</li>
<li>跨通道的信息交互<br>&#160; &#160; &#160; &#160;实现降维和升维的操作其实就是 channel 间信息的线性组合变化</li>
<li>减少卷积核参数，计算量。</li>
<li>计算量计算公式为卷积核的尺寸 ( D_K \times D_K \times M )、卷积核个数 ( N )，以及输出特征图尺寸 ( D_F \times D_F ) 的乘积：<br>[\text{计算量} &#x3D; D_K \times D_K \times M \times N \times D_F \times D_F]</li>
</ul>
<p><strong>14.LeNet继承nn.Module的原因：</strong></p>
<ul>
<li>nn.Module能够自动跟踪和管理神经网络中的参数（如权重和偏置），将它们存储在model.parameters()中，方便后续用于优化器和梯度更新。LeNet继承nn.Module后，可以自动享受这一参数管理的便利。</li>
<li>nn.Module提供了train()和eval()方法，分别用于设置模型为训练模式和推理（评估）模式。这会影响到一些特定层（如Dropout和BatchNorm）的行为。例如，在训练模式下，Dropout层会保留随机性；而在推理模式下，Dropout层会关闭。LeNet继承nn.Module后，可以方便地切换模型的训练模式和推理模式</li>
<li>nn.Module支持模型的保存和加载，LeNet继承nn.Module后，可以方便地实现模型的保存和加载功能</li>
</ul>
<p><strong>15.深度可分离卷积(DW+PW卷积)：</strong><br><strong>深度可分离卷积 &#x3D; 深度卷积 + 逐点卷积</strong></p>
<ol>
<li>**深度卷积(DW)**：<ul>
<li>使用单通道的卷积核对每一个输入通道进行独立卷积操作。即每个输入通道单独使用一个卷积核进行处理，结果是每个通道的输出特征图。</li>
</ul>
</li>
<li>**逐点卷积(PW)**：<ul>
<li>将深度卷积后的结果通过 (1 \times 1) 的卷积核汇聚成一个输出特征图。这相当于对所有通道的输出进行合并，生成最终的输出。</li>
</ul>
</li>
<li><strong>与标准卷积的关系</strong>：<ul>
<li>深度可分离卷积的两个步骤组合起来，产生的效果与标准卷积相同，但计算量更小。</li>
</ul>
</li>
<li><strong>优势</strong>：<ul>
<li><strong>计算量小</strong>：深度可分离卷积显著减少了参数量和计算量，特别是在处理高维数据时。</li>
<li><strong>效率高</strong>：通过分离深度卷积和逐点卷积，可以提高模型的运行效率，常用于移动设备和计算资源有限的场景（例如MobileNet）。</li>
</ul>
</li>
</ol>
<h3 id="深度可分离卷积"><a href="#深度可分离卷积" class="headerlink" title="深度可分离卷积"></a><a target="_blank" rel="noopener" href="https://blog.csdn.net/m0_37605642/article/details/134174749?ops_request_misc=%257B%2522request%255Fid%2522%253A%25229EF6D791-787C-4E72-B66B-F98EEACDB783%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&request_id=9EF6D791-787C-4E72-B66B-F98EEACDB783&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_positive~default-1-134174749-null-null.142%5Ev100%5Epc_search_result_base7&utm_term=%E6%B7%B1%E5%BA%A6%E5%8F%AF%E5%88%86%E7%A6%BB%E5%8D%B7%E7%A7%AF&spm=1018.2226.3001.4187">深度可分离卷积</a></h3><p><strong>16.转置卷积(反卷积或逆卷积):</strong><br>&#160; &#160; &#160; &#160;如果输入长宽大于输出，则为标准卷积；如果输出长宽大于输入，则为转置卷积。<br>&#160; &#160; &#160; &#160;对于转置卷积公式：<br>&#160; &#160; &#160; &#160;假设输入特征图的大小为 (H_{\text{in}} \times W_{\text{in}})，卷积核的大小为 (K_{\text{h}} \times K_{\text{w}})，步长为 (S)，填充为 (P)，则输出特征图的大小为：<br>[H_{\text{out}} &#x3D; S \times (H_{\text{in}} - 1) + K_{\text{h}} - 2P]<br>[W_{\text{out}} &#x3D; S \times (W_{\text{in}} - 1) + K_{\text{w}} - 2P]<br>&#160; &#160; &#160; &#160;转置卷积通过上采样，即将卷积核对应在输出结果上，从左上角开始，与输入的卷积核的数字一一相乘，填充在输出上，而后根据步长移动，若有某一位置被重叠，则将输出相加，进而实现了特征图空间维度的扩张</p>
<p><strong>17.shortcut和concat:</strong></p>
<ul>
<li>Shortcut 连接直接将前一层的输出与后一层的输出相加，通过残差学习的方式，帮助网络更轻松地学习到 恒等映射（identity mapping），从而解决了梯度消失或梯度爆炸的问题，加速了网络的训练过程。这意味着在残差连接中，输入和输出的特征图大小应该相同，以便可以直接将它们相加。</li>
<li>Concat 连接通常用于连接具有不同特征图大小的两个层，以实现跨层信息传递，Concat 连接将两个特征图在通道维度上进行拼接，将它们串联在一起形成一个更大的特征图，连接的两个特征图维度可以不同(宽度和高度)，但它们的通道数必须相同。</li>
</ul>
<p><strong>18.多种模型:</strong></p>
<h5 id="a-AlexNet"><a href="#a-AlexNet" class="headerlink" title="a.AlexNet"></a>a.AlexNet</h5><p>&#160; &#160; &#160; &#160;有5层卷积层和3层全连接层（FC 层），并引入了一些重要的创新来降低了过拟合风险，包括激活函数、Dropout 正则化和重叠池化<br>&#160; &#160; &#160; &#160;当然，其缺点就是参数多，计算资源大<br>&#160; &#160; &#160; &#160;层叠池化:即池化层的步长与池化核（我起的名字）不同，不像LeNet中均为2<br>&#160; &#160; &#160; &#160;dropout：通过给予神经元一定的失活概率来减少神经元的数量，防止过拟合（这个在LeNet中也会用到），当丢失率为0.5 时，Dropout会有最强的正则化效果.也有公式，但过于复杂不做展示</p>
<h3 id="AlexNet"><a href="#AlexNet" class="headerlink" title="AlexNet"></a><a target="_blank" rel="noopener" href="https://blog.csdn.net/guzhao9901/article/details/118552085?ops_request_misc=%257B%2522request%255Fid%2522%253A%252232F16701-19E2-4E63-91EB-24818A849330%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&request_id=32F16701-19E2-4E63-91EB-24818A849330&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_positive~default-1-118552085-null-null.142%5Ev100%5Epc_search_result_base8&utm_term=alexnet&spm=1018.2226.3001.4187">AlexNet</a></h3><h5 id="b-VGG16"><a href="#b-VGG16" class="headerlink" title="b.VGG16"></a>b.VGG16</h5><p>&#160; &#160; &#160; &#160;16表示16层，为1，3个卷积层和3个全连接层，都是由小卷积核、小池化核、ReLU组合，模型简洁</p>
<h3 id="VGG16"><a href="#VGG16" class="headerlink" title="VGG16"></a><a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_42012782/article/details/123222042?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522C3C5987A-91B4-4EFB-A598-9EC458CCC479%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&request_id=C3C5987A-91B4-4EFB-A598-9EC458CCC479&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_click~default-1-123222042-null-null.142%5Ev100%5Epc_search_result_base8&utm_term=VGG-16&spm=1018.2226.3001.4187">VGG16</a></h3><h5 id="c-ResNet网络"><a href="#c-ResNet网络" class="headerlink" title="c.ResNet网络"></a>c.ResNet网络</h5><p>&#160; &#160; &#160; &#160;为了解决梯度消失与梯度爆炸问题，提出了ResNet网络（残差网络），简单说来就是跳过某些层进行网络训练，来减少层与层之间的联系，进而防止了上述问题(理解就行)<br>&#160; &#160; &#160; &#160;ResNet的网络结构通常由多个残差块（Residual Block）堆叠而成，每个残差块包含多个卷积层、批量归一化层（Batch Normalization）和激活函数层。残差块的核心是跳跃连接，它将输入直接连接到输出，与经过卷积层处理的特征相加。</p>
<ul>
<li>输入层：通常是一个卷积层，用于提取输入图像的低级特征。</li>
<li>残差块：具体结构：1×1卷积降维，3×3卷积DW，1×1卷积升维，激活函数为ReLU(6)<ul>
<li>一个或多个卷积层，用于进一步提取特征。</li>
<li>包含一个跳跃连接，将输入直接添加到残差块的输出上（即Shortcut连接，具体可见MobileNet）。</li>
<li>可以包含批量归一化层和激活函数层，以提高网络的稳定性和训练效率。</li>
</ul>
</li>
<li>全连接层：在网络的最后部分，通常包含一个或多个全连接层，用于将提取的特征映射到输出类别上。</li>
</ul>
<h3 id="ResNet网络"><a href="#ResNet网络" class="headerlink" title="ResNet网络"></a><a target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_44001371/article/details/134192776?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522BC20099F-F864-4A6A-A463-61BBB2D49D57%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&request_id=BC20099F-F864-4A6A-A463-61BBB2D49D57&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_positive~default-1-134192776-null-null.142%5Ev100%5Epc_search_result_base8&utm_term=Resnet&spm=1018.2226.3001.4187">ResNet网络</a></h3><h5 id="d-Inception"><a href="#d-Inception" class="headerlink" title="d.Inception:"></a>d.Inception:</h5><p>增加网络的深度(往往同时增加每层神经元数量) 容易导致以下几个方面的问题.</p>
<pre><code>(1) 导致神经网络参数的数量过多,网络不容易训练,容易出现过拟合,需要更 多的训练数据. 然而训练数据并不容易获得,尤其是需要人工标记样本数据时就更 难了.所以如何在增加网络规模的同时尽可能地减少参数的数量是首先要考虑的 问题.

(2 )增大了网络的规模(更深,更宽) , 需要消耗大量的计算资源,需要&quot;有 效&quot;和&quot;充分&quot;地使用计算资源.以两个卷积堆叠为例,随着卷积层的过滤器线性增 加,所需要的计算资源与过滤器个数的平方成正比.如果所增加的计算资源没有 被&quot;有效&quot;地使用,如所有的权重于0(但不等于0),那么这些计算量不会带来准确率 的提高,

(3)当网络的深度达到一定程度之后,浅层神经网络容易出现梯度弥散的问题, 这是因为,误差反向传播的时候,随着深度的增加梯度会迅速变小,从而导致权重参 数变化缓慢,模型无法收敛.
</code></pre>
<p>而Inception就是将多个卷积或池化操作放在一起组装成一个网络模块，进而以模块设计网络<br><img src="/f3e3e7343e96d1a8b4bf5dffc374fc6d.png" alt="alt text"><br>一般就是由以下层组成</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">branch1:Conv1×1 , stride=1</span><br><span class="line">branch2:Conv3×3, stride=1, padding=1</span><br><span class="line">branch3:Conv5×5, stride=1, padding=2</span><br><span class="line">branch4:MaxPool3×3, stride=1, padding=1</span><br></pre></td></tr></table></figure>
<h3 id="Inception"><a href="#Inception" class="headerlink" title="Inception"></a><a target="_blank" rel="noopener" href="https://blog.csdn.net/ximu__l/article/details/129495171?ops_request_misc=%257B%2522request%255Fid%2522%253A%25228a4bc5a5cdbf0464b6bbf8c33f384313%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&request_id=8a4bc5a5cdbf0464b6bbf8c33f384313&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_positive~default-1-129495171-null-null.142%5Ev101%5Epc_search_result_base7&utm_term=Inception&spm=1018.2226.3001.4187">Inception</a></h3><h5 id="e-MobileNet"><a href="#e-MobileNet" class="headerlink" title="e.MobileNet:"></a>e.MobileNet:</h5><p>MobileNetV1:<br>(相比于传统卷积神经网络，在准确率小幅降低的前提下大大减少模型参数与运算量)</p>
<ul>
<li>运用深度可分离来减少参数</li>
<li>增加超参数α，β（α是控制卷积层卷积核个数的倍率，β是控制输入图像的大小），这是人为设置的，不是学习优化来的<br>[N_{\text{实际}} &#x3D; \alpha \cdot N_{\text{默认}}]  <ul>
<li>将卷积核的个数减少后，能保证准确率小幅下降的情况下，减少运算量</li>
<li>适当减少输入图像的大小，能保证准确率小幅下降的情况下，减少运算量</li>
</ul>
</li>
</ul>
<p>MobilenetV2:<br>（相比于MobileNet V1网络，准确率更高，模型更小）</p>
<ul>
<li>Inverted Residuals（倒残差结构）<br>不同于ResNet的残差结构，此具体结构为1×1卷积升维，3×3卷积DW，1×1卷积降维，激活函数为ReLU(6)。<br><img src="/89ff091c6be20ac0d38768dda58bd55f-1.png" alt="alt text"></li>
<li>Linear Bottlenecks（线性瓶颈结构）<br>Linear Bottlenecks是针对倒残差结构最后一个1×1的卷积层，使用了线性的激活函数，而不是ReLU激活函数。</li>
<li>Shortcut 连接(残差连接)<br>&#160; &#160; &#160; &#160;Shortcut 连接的核心思想是直接将输入特征图加到某一层的输出上，从而让信息能够绕过某些层直接传递(就是那一步相加的过程)<br>&#160; &#160; &#160; &#160;而在 MobileNetV2 中，shortcut 连接的使用有一些特别之处：即只有在 stride&#x3D;1 且输入输出特征图的形状相同（即宽度、高度和通道数相同）时，才使用 shortcut 连接。<br>&#160; &#160; &#160; &#160;目的：在倒残差结构中，shortcut 连接有助于将信息传递到网络的后续层，尤其是当输入特征图和输出特征图有相同尺寸时。</li>
</ul>
<p>MobileNetV3：</p>
<ul>
<li>更新Block（bneck），即Bottlenecks<ul>
<li>加入SE（Squeeze-and-Excitation）模块（通道的注意力机制模块）</li>
<li>更新了激活函数</li>
</ul>
</li>
<li>重新设计耗时层结构<ul>
<li>减少第一个卷积层的卷积核个数（32变为16）</li>
<li>精简Last Stage</li>
</ul>
</li>
</ul>
<h3 id="MobileNet"><a href="#MobileNet" class="headerlink" title="MobileNet"></a><a target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_45827876/article/details/130746759?ops_request_misc=%257B%2522request%255Fid%2522%253A%25225aee863592959a28cb36e237845d8bca%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&request_id=5aee863592959a28cb36e237845d8bca&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_click~default-2-130746759-null-null.142%5Ev101%5Epc_search_result_base7&utm_term=mobilenet&spm=1018.2226.3001.4187">MobileNet</a></h3><h5 id="f-EfficientNet"><a href="#f-EfficientNet" class="headerlink" title="f.EfficientNet"></a>f.EfficientNet</h5><ul>
<li>MBconvk模块:主要由一个1x1的普通卷积（升维作用，包含BN和Swish），一个$k∗k$的Depthwise Conv卷积（包含BN和Swish深度可分离卷积），k的具体值可看EfficientNet-B0的网络框架主要有3x3和5x5两种情况，一个SE模块，一个1x1的普通卷积（降维作用，包含BN），一个Droupout层构成</li>
<li>SE注意力机制模块:同MobileNet模型中的一样</li>
<li>深度可分离卷积</li>
</ul>
<h3 id="EfficientNet"><a href="#EfficientNet" class="headerlink" title="EfficientNet"></a><a target="_blank" rel="noopener" href="https://blog.csdn.net/J_oshua/article/details/138583004?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522bd05088190cf676594083f346cffc8e7%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&request_id=bd05088190cf676594083f346cffc8e7&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_positive~default-1-138583004-null-null.142%5Ev101%5Epc_search_result_base7&utm_term=efficientnet&spm=1018.2226.3001.4187">EfficientNet</a></h3></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="http://example.com">Cx330</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="http://example.com/2026/01/09/%E5%8D%B7%E7%A7%AF/">http://example.com/2026/01/09/%E5%8D%B7%E7%A7%AF/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来源 <a href="http://example.com" target="_blank">维娜丝小课堂</a>！</span></div></div><div class="tag_share"><div class="post-share"><div class="social-share" data-image="/img/butterfly-icon.png" data-sites="facebook,x,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.6/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.6/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related full-width" href="/2026/01/08/hello-world/" title="Hello World"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-right"><div class="info-1"><div class="info-item-1">下一篇</div><div class="info-item-2">Hello World</div></div><div class="info-2"><div class="info-item-1">Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new &quot;My New Post&quot;  More info: Writing Run server1$ hexo server  More info: Server Generate static files1$ hexo generate  More info: Generating Deploy to remote sites1$ hexo deploy  More info: Deployment </div></div></div></a></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="/img/butterfly-icon.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">Cx330</div><div class="author-info-description">生命久如暗室,不妨碍我明写春诗</div><div class="site-data"><a href="/archives/"><div class="headline">文章</div><div class="length-num">2</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">0</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">0</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/xxxxxx"><i class="fab fa-github"></i><span>Follow Me</span></a></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%B7%B1%E5%BA%A6%E5%8F%AF%E5%88%86%E7%A6%BB%E5%8D%B7%E7%A7%AF"><span class="toc-number">1.</span> <span class="toc-text">深度可分离卷积</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#a-AlexNet"><span class="toc-number">1.0.1.</span> <span class="toc-text">a.AlexNet</span></a></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#AlexNet"><span class="toc-number">2.</span> <span class="toc-text">AlexNet</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#b-VGG16"><span class="toc-number">2.0.1.</span> <span class="toc-text">b.VGG16</span></a></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#VGG16"><span class="toc-number">3.</span> <span class="toc-text">VGG16</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#c-ResNet%E7%BD%91%E7%BB%9C"><span class="toc-number">3.0.1.</span> <span class="toc-text">c.ResNet网络</span></a></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#ResNet%E7%BD%91%E7%BB%9C"><span class="toc-number">4.</span> <span class="toc-text">ResNet网络</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#d-Inception"><span class="toc-number">4.0.1.</span> <span class="toc-text">d.Inception:</span></a></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Inception"><span class="toc-number">5.</span> <span class="toc-text">Inception</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#e-MobileNet"><span class="toc-number">5.0.1.</span> <span class="toc-text">e.MobileNet:</span></a></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#MobileNet"><span class="toc-number">6.</span> <span class="toc-text">MobileNet</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#f-EfficientNet"><span class="toc-number">6.0.1.</span> <span class="toc-text">f.EfficientNet</span></a></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#EfficientNet"><span class="toc-number">7.</span> <span class="toc-text">EfficientNet</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2026/01/09/%E5%8D%B7%E7%A7%AF/" title="卷积">卷积</a><time datetime="2026-01-09T02:53:15.000Z" title="发表于 2026-01-09 10:53:15">2026-01-09</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2026/01/08/hello-world/" title="Hello World">Hello World</a><time datetime="2026-01-08T10:37:17.674Z" title="发表于 2026-01-08 18:37:17">2026-01-08</time></div></div></div></div></div></div></main><footer id="footer"><div class="footer-other"><div class="footer-copyright"><span class="copyright">&copy;&nbsp;2025 - 2026 By Cx330</span><span class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo 7.3.0</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly 5.5.2</a></span></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="日间和夜间模式切换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=5.5.2"></script><script src="/js/main.js?v=5.5.2"></script><div class="js-pjax"></div><script async data-pjax src="/"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><i class="fas fa-spinner fa-pulse" id="loading-status" hidden="hidden"></i><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="text-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据加载中</span></div><div class="local-search-input"><input placeholder="搜索文章..." type="text"/></div><hr/><div id="local-search-results"></div><div class="ais-Pagination" id="local-search-pagination" style="display:none;"><ul class="ais-Pagination-list"></ul></div><div id="local-search-stats"></div></div><div id="search-mask"></div><script src="/js/search/local-search.js?v=5.5.2"></script></div></div></body></html>